import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class MaskedLinear(nn.Linear):
  def __init__(self, in_features, out_features, mask, bias=True):
    super().__init__(in_features, out_features, bias)
    self.register_buffer('mask', mask)

  def forward(self, x):
        return F.linear(x, self.weight*self.mask, self.bias)

  def create_masks(
    input_size, hidden_size, n_hidden, input_order="sequential", input_degrees=None
):
    # MADE paper sec 4:
    # degrees of connections between layers -- ensure at most in_degree - 1 connections
    degrees = []

    # set input degrees to what is provided in args (the flipped order of the previous layer in a stack of mades);
    # else init input degrees based on strategy in input_order (sequential or random)
    if input_order == "sequential":
        degrees += (
            [torch.arange(input_size)] if input_degrees is None else [input_degrees]
        )
        for _ in range(n_hidden):
            degrees += [torch.arange(hidden_size) % (input_size - 1)]
        degrees += (
            [torch.arange(input_size) % input_size - 1]
            if input_degrees is None
            else [input_degrees % input_size - 1]
        )

    elif input_order == "random":
        degrees += (
            [torch.randperm(input_size)] if input_degrees is None else [input_degrees]
        )
        for _ in range(n_hidden):
            min_prev_degree = min(degrees[-1].min().item(), input_size - 1)
            degrees += [torch.randint(min_prev_degree, input_size, (hidden_size,))]
        min_prev_degree = min(degrees[-1].min().item(), input_size - 1)
        degrees += (
            [torch.randint(min_prev_degree, input_size, (input_size,)) - 1]
            if input_degrees is None
            else [input_degrees - 1]
        )

    # construct masks
    masks = []
    for (d0, d1) in zip(degrees[:-1], degrees[1:]):
        masks += [(d1.unsqueeze(-1) >= d0.unsqueeze(0)).float()]

    return masks, degrees[0]

class MADE():
  def __init__(self, in_features, hidden_features, n_hidden, out_features):
        super(MADE, self).__init__()
        self.in_features = in_features
        self.hidden_features = hidden_features
        self.n_hidden = n_hidden
        self.out_features = out_features

        masks,self.in_features=create_masks(in_features,hidden_features)
        self.net_input = MaskedLinear(in_features, hidden_features, masks[0])

        self.net = []
        for m in masks[1:-1]:
            self.net.append(nn.ReLU())
            self.net.append(MaskedLinear(hidden_features, hidden_features, m))

        self.net.append(nn.ReLU())
        self.net.append(MaskedLinear(hidden_features, out_features, masks[-1]))

        self.net = nn.Sequential(*self.net)

    def compute_loss(self, x, output, criterion):
        # binary cross entropy loss
        loss = criterion(output, x)
        return {"loss": loss}
